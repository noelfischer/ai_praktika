{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOydnCCUUH6mavCAYaYA6On",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/noelfischer/ai_praktika/blob/main/4/task_2_building_locks_problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2.1: Manually Calculate Output of Simple Feedforward Neural Network with ReLU Activation\n",
        "Problem:\n",
        "\n",
        "Given a neural network with:\n",
        "\n",
        "    2 input neurons,\n",
        "    1 hidden layer with 3 neurons (ReLU activation),\n",
        "    1 output neuron (ReLU activation), calculate the output manually.\n",
        "\n",
        "Let's denote:\n",
        "\n",
        "    Input vector x=[x1,x2]x=[x1​,x2​]\n",
        "    Weights from input to hidden layer W1=[[w11,w12],[w21,w22],[w31,w32]]W1​=[[w11​,w12​],[w21​,w22​],[w31​,w32​]]\n",
        "    Biases for hidden layer b1=[bh1,bh2,bh3]b1​=[bh1​,bh2​,bh3​]\n",
        "    Weights from hidden layer to output layer W2=[wout1,wout2,wout3]W2​=[wout1​,wout2​,wout3​]\n",
        "    Bias for output layer b2=[bout]b2​=[bout​]\n",
        "\n",
        "Using the ReLU function ReLU(x)=max⁡(0,x)ReLU(x)=max(0,x).\n",
        "Solution:"
      ],
      "metadata": {
        "id": "-Pzn3Oq3lWff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ri4j8M5fr4e",
        "outputId": "fa88175b-720d-431a-dd98-3f0d924b4dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden layer activation: [0.  1.6 0.2]\n",
            "Output layer activation: [0.]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define inputs, weights, and biases\n",
        "x = np.array([1.0, 2.0])  # Input vector\n",
        "W1 = np.array([[0.5, -0.6], [0.3, 0.8], [-0.2, 0.1]])  # Weights from input to hidden layer\n",
        "b1 = np.array([0.1, -0.3, 0.2])  # Biases for hidden layer\n",
        "\n",
        "W2 = np.array([0.7, -1.2, 0.5])  # Weights from hidden layer to output\n",
        "b2 = np.array([0.3])  # Bias for output layer\n",
        "\n",
        "# Manual Calculation of Hidden Layer Activation\n",
        "z_h = np.dot(W1, x) + b1\n",
        "a_h = np.maximum(0, z_h)  # Apply ReLU activation\n",
        "print(\"Hidden layer activation:\", a_h)\n",
        "\n",
        "# Manual Calculation of Output Layer Activation\n",
        "z_out = np.dot(W2, a_h) + b2\n",
        "a_out = np.maximum(0, z_out)  # Apply ReLU activation\n",
        "print(\"Output layer activation:\", a_out)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2.2: Calculate Output with Sigmoid Activation\n",
        "Problem:\n",
        "\n",
        "A simple feedforward network with:\n",
        "\n",
        "    2 input neurons,\n",
        "    1 hidden layer with 1 neuron (Sigmoid activation),\n",
        "    1 output neuron (Sigmoid activation).\n",
        "\n",
        "Given input vector x=[x1,x2]x=[x1​,x2​], weights, and biases, calculate the output.\n",
        "Solution:"
      ],
      "metadata": {
        "id": "bMFog4Bqlgr3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input, weights, and biases\n",
        "x = np.array([1.0, 2.0])  # Input vector\n",
        "W1 = np.array([0.4, -0.7])  # Weights for input to hidden layer\n",
        "b1 = -0.1  # Bias for hidden layer\n",
        "\n",
        "W2 = np.array([0.6])  # Weight for hidden layer to output layer\n",
        "b2 = 0.2  # Bias for output layer\n",
        "\n",
        "# Sigmoid function\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "# Calculate hidden layer activation\n",
        "z_h = np.dot(W1, x) + b1\n",
        "a_h = sigmoid(z_h)  # Sigmoid activation\n",
        "print(\"Hidden layer activation:\", a_h)\n",
        "\n",
        "# Calculate output layer activation\n",
        "z_out = np.dot(W2, a_h) + b2\n",
        "a_out = sigmoid(z_out)  # Sigmoid activation\n",
        "print(\"Output layer activation:\", a_out)\n"
      ],
      "metadata": {
        "id": "BWtJ52FEljXr",
        "outputId": "3b03a89c-451a-4e9c-9fe1-69378d335a5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hidden layer activation: 0.24973989440488245\n",
            "Output layer activation: [0.58657973]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise 2.3: Weights Increase with Depth vs. Width\n",
        "Problem:\n",
        "\n",
        "For a network with:\n",
        "\n",
        "    Input dimension Di=1Di​=1,\n",
        "    Output dimension Do=1Do​=1,\n",
        "    K=10K=10 layers,\n",
        "    D=10D=10 hidden units per layer.\n",
        "\n",
        "Determine whether increasing depth or width by 1 results in a greater increase in the number of weights.\n",
        "Solution:\n",
        "\n",
        "If we increase depth by 1:\n",
        "\n",
        "    Total weights: 11×102=110011×102=1100\n",
        "\n",
        "If we increase width by 1 (from 10 to 11 hidden units per layer):\n",
        "\n",
        "    Total weights: 10×112=121010×112=1210\n",
        "\n",
        "Conclusion: Increasing width by 1 adds more weights than increasing depth by 1, making width expansion more impactful on the number of weights.\n",
        "Exercise 2.4: Handling Different Output Scales in Multivariate Regression\n",
        "Problem:\n",
        "\n",
        "In a multivariate regression problem where one output is height (in meters) and another is weight (in kilos), the range of values may differ significantly.\n",
        "Solution:\n",
        "\n",
        "This discrepancy can cause issues in training due to different gradient scales. Solutions include:\n",
        "\n",
        "    Normalization: Standardize each output feature to have mean 0 and standard deviation 1.\n",
        "    Weighted Loss Function: Adjust the loss function to balance importance.\n",
        "    Scaled Outputs: Use a scaling factor to adjust predictions before and after training.\n",
        "\n",
        "Exercise 2.5: Comparing SGD and Adam Optimizers\n",
        "Part (a): Explanation of Optimizers\n",
        "\n",
        "    SGD: Updates weights based on the gradient of the loss function. With momentum, it helps escape small local minima.\n",
        "    Adam: Uses adaptive learning rates with momentum, using two running averages of gradients. It updates weights more flexibly and is generally faster in practice.\n",
        "\n",
        "Part (b): Advantages and Disadvantages\n",
        "\n",
        "    SGD:\n",
        "        Pros: Simpler, less prone to overfitting, often preferred for larger datasets.\n",
        "        Cons: Slower convergence.\n",
        "    Adam:\n",
        "        Pros: Faster convergence, adaptive learning rates.\n",
        "        Cons: Can overfit, sensitive to parameter settings.\n",
        "\n",
        "Part (c): Adam Hyperparameters\n",
        "\n",
        "    Beta: Controls decay rates for moment estimates (default values are usually effective).\n",
        "    Gamma: Learning rate multiplier (not in original Adam but seen in variations).\n",
        "    Epsilon: Prevents division by zero, with smaller values resulting in more precise updates.\n",
        "\n",
        "Example code snippet for Adam in PyTorch:"
      ],
      "metadata": {
        "id": "y0cxEG5ylryZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define model, loss, and optimizer\n",
        "model = nn.Linear(2, 1)\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08)\n",
        "\n",
        "# Training step\n",
        "input_data = torch.tensor([[1.0, 2.0]], requires_grad=True)\n",
        "target = torch.tensor([[0.5]])\n",
        "optimizer.zero_grad()\n",
        "output = model(input_data)\n",
        "loss = loss_fn(output, target)\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Updated weights:\", model.weight)\n",
        "print(\"Updated bias:\", model.bias)\n"
      ],
      "metadata": {
        "id": "cq3NuAbqlwJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code demonstrates a basic forward and backward pass with Adam, showing how the optimizer updates weights based on gradients."
      ],
      "metadata": {
        "id": "nYnxNPpilv-K"
      }
    }
  ]
}